{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db4492bc-7b66-4b04-9e2f-5a9a76945029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b96cf9c7-9ac1-4984-aac0-53c31071eeb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data for class 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-06 10:52:37.528 python[1321:17834] +[IMKClient subclass]: chose IMKClient_Legacy\n",
      "2024-11-06 10:52:37.528 python[1321:17834] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data for class 1\n",
      "Collecting data for class 2\n",
      "Collecting data for class 3\n",
      "Collecting data for class 4\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = './data'\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "number_of_classes = 5\n",
    "dataset_size = 1000\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "for j in range(number_of_classes):\n",
    "    if not os.path.exists(os.path.join(DATA_DIR, str(j))):\n",
    "        os.makedirs(os.path.join(DATA_DIR, str(j)))\n",
    "\n",
    "    print('Collecting data for class {}'.format(j))\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        cv2.putText(frame, 'Ready? Press \"Q\" ! :)', (100, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 255, 0), 3,cv2.LINE_AA)\n",
    "        cv2.imshow('frame', frame)\n",
    "        if cv2.waitKey(25) == ord('q'):\n",
    "            break\n",
    "\n",
    "    counter = 0\n",
    "    while counter < dataset_size:\n",
    "        ret, frame = cap.read()\n",
    "        cv2.imshow('frame', frame)\n",
    "        cv2.waitKey(25)\n",
    "        cv2.imwrite(os.path.join(DATA_DIR, str(j), '{}.jpg'.format(counter)), frame)\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8d1e828-cc19-4e34-badb-fd8e23b4b7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1730871233.115874   17834 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M3 Pro\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load image at path: ./data/3/.ipynb_checkpoints\n",
      "Data processing complete and saved to 'data.pickle'\n"
     ]
    }
   ],
   "source": [
    "## Creating the marking for the hand\n",
    "# Initialize Mediapipe Hands and Drawing Utilities\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# Set up the Hands model in static image mode\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "\n",
    "# Directory containing the image data\n",
    "DATA_DIR = './data'\n",
    "\n",
    "# Prepare lists to hold data and labels\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Loop through each directory and image file in DATA_DIR\n",
    "for dir_ in os.listdir(DATA_DIR):\n",
    "    for img_path in os.listdir(os.path.join(DATA_DIR, dir_)):\n",
    "        data_aux = []\n",
    "        x_ = []\n",
    "        y_ = []\n",
    "\n",
    "        # Construct full path to the image\n",
    "        img_path_full = os.path.join(DATA_DIR, dir_, img_path)\n",
    "        \n",
    "        # Read the image\n",
    "        img = cv2.imread(img_path_full)\n",
    "\n",
    "        # Check if image is successfully loaded\n",
    "        if img is None:\n",
    "            print(f\"Failed to load image at path: {img_path_full}\")\n",
    "            continue\n",
    "\n",
    "        # Convert the image to RGB for Mediapipe processing\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Process the image to detect hands and landmarks\n",
    "        results = hands.process(img_rgb)\n",
    "        if results.multi_hand_landmarks:\n",
    "            # Loop through each detected hand\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Extract landmarks and store x, y coordinates\n",
    "                for i in range(len(hand_landmarks.landmark)):\n",
    "                    x = hand_landmarks.landmark[i].x\n",
    "                    y = hand_landmarks.landmark[i].y\n",
    "                    x_.append(x)\n",
    "                    y_.append(y)\n",
    "\n",
    "                # Normalize landmarks by subtracting minimum x, y values\n",
    "                for i in range(len(hand_landmarks.landmark)):\n",
    "                    x = hand_landmarks.landmark[i].x\n",
    "                    y = hand_landmarks.landmark[i].y\n",
    "                    data_aux.append(x - min(x_))\n",
    "                    data_aux.append(y - min(y_))\n",
    "\n",
    "            # Append processed data and labels\n",
    "            data.append(data_aux)\n",
    "            labels.append(dir_)\n",
    "\n",
    "# Save processed data and labels to a pickle file\n",
    "with open('data.pickle', 'wb') as f:\n",
    "    pickle.dump({'data': data, 'labels': labels}, f)\n",
    "\n",
    "# Release resources\n",
    "hands.close()\n",
    "print(\"Data processing complete and saved to 'data.pickle'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "414e2b86-db24-44d5-86bc-f189407eba43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from file...\n",
      "99.47423764458465% of samples were classified correctly!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wj/m49ss9pd6f9_585csx_lq3_00000gn/T/ipykernel_2396/315499004.py:61: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    }
   ],
   "source": [
    "##Model train\n",
    "# Load data\n",
    "data_dict = pickle.load(open('./data.pickle', 'rb'))\n",
    "data = data_dict['data']\n",
    "labels = data_dict['labels']\n",
    "\n",
    "# Determine the fixed length for each sample (e.g., 42 coordinates if 21 landmarks with x and y each)\n",
    "fixed_length = 42  # Adjust this based on your landmarks\n",
    "\n",
    "# Pad or truncate data to fixed length\n",
    "data_padded = []\n",
    "for sample in data:\n",
    "    if len(sample) < fixed_length:\n",
    "        sample += [0] * (fixed_length - len(sample))\n",
    "    else:\n",
    "        sample = sample[:fixed_length]\n",
    "    data_padded.append(sample)\n",
    "\n",
    "# Convert to numpy array\n",
    "data_padded = np.array(data_padded)\n",
    "labels = np.asarray(labels)\n",
    "\n",
    "# Encode labels to numeric values\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Split data\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_padded, labels_encoded, test_size=0.2, shuffle=True, stratify=labels_encoded)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Define model parameters\n",
    "input_size = fixed_length\n",
    "hidden_size = 64\n",
    "output_size = len(np.unique(labels_encoded))\n",
    "\n",
    "# Define your PyTorch model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Load model if exists, otherwise train\n",
    "model_path = 'model.pth'\n",
    "model = SimpleNN(input_size, hidden_size, output_size)\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Loading model from file...\")\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "else:\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 1000\n",
    "    for epoch in range(num_epochs):\n",
    "        outputs = model(x_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Save the model's state dictionary\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(\"Model trained and saved to file.\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(x_test_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    accuracy = accuracy_score(predicted.numpy(), y_test)\n",
    "\n",
    "print('{}% of samples were classified correctly!'.format(accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f03aa89-32cd-4e5c-976e-584c487c3aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wj/m49ss9pd6f9_585csx_lq3_00000gn/T/ipykernel_1010/3507431636.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('./model.pth'))\n",
      "I0000 00:00:1732078843.182878   10673 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M3 Pro\n"
     ]
    }
   ],
   "source": [
    "## Model testing\n",
    "# Define the SimpleNN class to match the saved model because pytorch only saves the the weights \n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Model parameters \n",
    "input_size = 42  # This should match the fixed length used in training\n",
    "hidden_size = 64\n",
    "output_size = 5  \n",
    "\n",
    "# Instantiate and load the model\n",
    "model = SimpleNN(input_size, hidden_size, output_size)\n",
    "model.load_state_dict(torch.load('./model.pth'))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize MediaPipe hands\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# Configure MediaPipe Hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, min_detection_confidence=0.3)\n",
    "\n",
    "# Label dictionary\n",
    "labels_dict = {0: \"Hello\", 1: \"Please\", 2: \"Thank you\", 3: \"OK\", 4: \"Thumbs up\"}\n",
    "\n",
    "while True:\n",
    "    data_aux = []\n",
    "    x_ = []\n",
    "    y_ = []\n",
    "\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    H, W, _ = frame.shape\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the image and detect hands\n",
    "    results = hands.process(frame_rgb)\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style()\n",
    "            )\n",
    "\n",
    "            # Collect x, y coordinates of each landmark\n",
    "            for landmark in hand_landmarks.landmark:\n",
    "                x = landmark.x\n",
    "                y = landmark.y\n",
    "                x_.append(x)\n",
    "                y_.append(y)\n",
    "\n",
    "            # Normalize landmarks and construct data_aux with 42 elements\n",
    "            for landmark in hand_landmarks.landmark:\n",
    "                data_aux.append(landmark.x - min(x_))\n",
    "                data_aux.append(landmark.y - min(y_))\n",
    "\n",
    "            # Ensure data_aux matches the expected input shape of the model\n",
    "            data_aux = data_aux[:42]  # Keep only 42 values if more were added\n",
    "\n",
    "            # Bounding box coordinates for hand\n",
    "            x1 = int(min(x_) * W) - 10\n",
    "            y1 = int(min(y_) * H) - 10\n",
    "            x2 = int(max(x_) * W) - 10\n",
    "            y2 = int(max(y_) * H) - 10\n",
    "\n",
    "            # Inference\n",
    "            with torch.no_grad():\n",
    "                inputs = torch.tensor(data_aux, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
    "                outputs = model(inputs)  # Pass the input through the model\n",
    "                predicted_index = torch.argmax(outputs, dim=1).item()  # Get predicted class index\n",
    "                predicted_character = labels_dict.get(predicted_index, \"Unknown\")  # Get label or default to \"Unknown\"\n",
    "\n",
    "            # Display bounding box and prediction on frame\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 0), 4)\n",
    "            cv2.putText(frame, predicted_character, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 0, 0), 3,cv2.LINE_AA)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('frame', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):  # Exit if 'q' key is pressed\n",
    "        break\n",
    "\n",
    "# Release the capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989d3a62-b08f-4061-bd2c-654854468980",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
